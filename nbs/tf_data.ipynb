{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vol/ml/apartin/projects/pdx-histo/nbs\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from time import time\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "fdir = Path.cwd()\n",
    "print(fdir)\n",
    "# sys.path.append(str(fdir/'../src'))\n",
    "sys.path.append(str(fdir/'..'))\n",
    "import src\n",
    "# from config import cfg\n",
    "from src.config import cfg\n",
    "from src.ml.scale import get_scaler\n",
    "from src.utils.utils import Params, dump_dict, read_lines, cast_list, Timer\n",
    "from src.datasets.tidy import split_data_and_extract_fea, extract_fea\n",
    "\n",
    "from src.tf_utils import (calc_records_in_tfr_folder, calc_examples_in_tfrecord, get_tfr_files,\n",
    "                          _float_feature, _bytes_feature, _int64_feature)\n",
    "from src.sf_utils import read_annotations, green, parse_tfrec_fn_rsp, create_tf_data\n",
    "from src.tfrecords import FEA_SPEC_RNA, FEA_SPEC_RSP_DRUG_PAIR\n",
    "\n",
    "from src.deephistopath.wsi import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_dir = cfg.SF_TFR_DIR_RSP_DRUG_PAIR/\"299px_302um\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6962, 4950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/ml/apartin/projects/pdx-histo/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (3,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "prjname = \"bin_rsp_drug_pairs_all_samples\"\n",
    "dataname = \"tidy_drug_pairs_all_samples\"\n",
    "prjdir = cfg.MAIN_PRJDIR/prjname\n",
    "annotations_file = cfg.DATA_PROCESSED_DIR/dataname/cfg.SF_ANNOTATIONS_FILENAME\n",
    "data = pd.read_csv(annotations_file)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_cols  = [c for c in data.columns if c.startswith(\"ge_\")]\n",
    "dd1_cols = [c for c in data.columns if c.startswith(\"dd1_\")]\n",
    "dd2_cols = [c for c in data.columns if c.startswith(\"dd2_\")]\n",
    "data = data.astype({\"image_id\": str, \"slide\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ge, use_dd1, use_dd2 = True, True, True\n",
    "\n",
    "# Scalers for each feature set\n",
    "ge_scaler, dd1_scaler, dd2_scaler = None, None, None\n",
    "\n",
    "if use_ge and len(ge_cols) > 0:\n",
    "    ge_scaler = get_scaler(data[ge_cols])\n",
    "\n",
    "if use_dd1 and len(dd1_cols) > 0:\n",
    "    dd1_scaler = get_scaler(data[dd1_cols])\n",
    "\n",
    "if use_dd2 and len(dd2_cols) > 0:\n",
    "    dd2_scaler = get_scaler(data[dd2_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tfrecords the folder: 6962\n",
      "Number of examples in the tfrecord: 617\n"
     ]
    }
   ],
   "source": [
    "# Summary of tfrecords\n",
    "tfr_files = sorted(str(p) for p in tfr_dir.glob('*.tfrec*'))\n",
    "print('Number of tfrecords the folder:', len(tfr_files))\n",
    "# calc_records_in_tfr_folder(tfr_dir=TFR_DIR)\n",
    "calc_examples_in_tfrecord(tfr_path=str(tfr_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Yitan's splits\n",
    "# --------------\n",
    "split_on = \"Group\"\n",
    "\n",
    "# import ipdb; ipdb.set_trace()\n",
    "splitdir = cfg.DATADIR/\"PDX_Transfer_Learning_Classification/Processed_Data/Data_For_MultiModal_Learning/Data_Partition\"\n",
    "# split_id = 0\n",
    "split_id = 2\n",
    "\n",
    "index_col_name = \"index\"\n",
    "tr_id = cast_list(read_lines(str(splitdir/f\"cv_{split_id}\"/\"TrainList.txt\")), int)\n",
    "vl_id = cast_list(read_lines(str(splitdir/f\"cv_{split_id}\"/\"ValList.txt\")), int)\n",
    "te_id = cast_list(read_lines(str(splitdir/f\"cv_{split_id}\"/\"TestList.txt\")), int)\n",
    "\n",
    "# Update ids\n",
    "tr_id = sorted(set(data[index_col_name]).intersection(set(tr_id)))\n",
    "vl_id = sorted(set(data[index_col_name]).intersection(set(vl_id)))\n",
    "te_id = sorted(set(data[index_col_name]).intersection(set(te_id)))\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"ge_cols\": ge_cols,\n",
    "          \"dd1_cols\": dd1_cols,\n",
    "          \"dd2_cols\": dd2_cols,\n",
    "          \"ge_scaler\": ge_scaler,\n",
    "          \"dd1_scaler\": dd1_scaler,\n",
    "          \"dd2_scaler\": dd2_scaler,\n",
    "          \"ge_dtype\": cfg.GE_DTYPE,\n",
    "          \"dd_dtype\": cfg.DD_DTYPE,\n",
    "          \"index_col_name\": index_col_name,\n",
    "          \"split_on\": split_on\n",
    "}\n",
    "tr_ge, tr_dd1, tr_dd2, tr_meta = split_data_and_extract_fea(data, ids=tr_id, **kwargs)\n",
    "vl_ge, vl_dd1, vl_dd2, vl_meta = split_data_and_extract_fea(data, ids=vl_id, **kwargs)\n",
    "te_ge, te_dd1, te_dd2, te_meta = split_data_and_extract_fea(data, ids=te_id, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sample names for T/V/E\n",
    "id_name = \"smp\"\n",
    "tr_smp_names = list(tr_meta[id_name].values)\n",
    "vl_smp_names = list(vl_meta[id_name].values)\n",
    "te_smp_names = list(te_meta[id_name].values)\n",
    "\n",
    "# TFRecords filenames\n",
    "tr_tfr_files = get_tfr_files(tfr_dir, tr_smp_names)  # training_tfrecords\n",
    "vl_tfr_files = get_tfr_files(tfr_dir, vl_smp_names)  # validation_tfrecords\n",
    "te_tfr_files = get_tfr_files(tfr_dir, te_smp_names)\n",
    "print('Total samples {}'.format(len(tr_tfr_files) + len(vl_tfr_files) + len(te_tfr_files)))\n",
    "\n",
    "# Missing tfrecords\n",
    "print('\\nThese samples miss a tfrecord ...\\n')\n",
    "print(data.loc[~data[id_name].isin(tr_smp_names + vl_smp_names + te_smp_names), ['smp', 'image_id']])\n",
    "\n",
    "train_tfrecords = tr_tfr_files\n",
    "val_tfrecords = vl_tfr_files\n",
    "test_tfrecords = te_tfr_files\n",
    "\n",
    "print(\"train TFRecords:\", len(tr_tfr_files))\n",
    "print(\"val   TFRecords:\", len(vl_tfr_files))\n",
    "print(\"test  TFRecords:\", len(te_tfr_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Read single tfrecord and explore single example\n",
    "# -----------------------------------------------\n",
    "\n",
    "# ds = tf.data.TFRecordDataset(tfr_files[0])\n",
    "ds = tf.data.TFRecordDataset(te_tfr_files[0])\n",
    "rec = next(ds.__iter__())\n",
    "features = tf.io.parse_single_example(rec, features=FEA_SPEC_RSP_DRUG_PAIR)  # returns features for a given example in a dict\n",
    "print('Feature types in the tfrecord example:\\n{}'.format(features.keys()))\n",
    "print('\\nBytes (image_raw):')\n",
    "img = tf.image.decode_jpeg(features['image_raw'], channels=3)\n",
    "print(tf.shape(img))\n",
    "print(img.numpy().shape)\n",
    "\n",
    "# plt.imshow(img.numpy());\n",
    "util.np_to_pil(img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(te_tfr_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_list = []\n",
    "rsp_list = []\n",
    "im_list = []\n",
    "idx_list = []\n",
    "\n",
    "# All tfrecord\n",
    "# ds = tf.data.TFRecordDataset(tfr_files)\n",
    "# ds = tf.data.TFRecordDataset(te_tfr_files)\n",
    "\n",
    "# ds = tf.data.Dataset.from_tensor_slices(te_tfr_files)\n",
    "# ds = ds.interleave(lambda x: tf.data.TFRecordDataset(x), cycle_length=4)\n",
    "\n",
    "# Subset of tfrecords\n",
    "# ds = tf.data.TFRecordDataset(te_tfr_files[0])  # label 1 (digestive/gastrointestinal)\n",
    "# ds = tf.data.TFRecordDataset(te_tfr_files[2])  # label 10 (squamous)\n",
    "ds = tf.data.TFRecordDataset(te_tfr_files[:3])  # get 3 tfrecords\n",
    "\n",
    "ds = ds.shuffle(buffer_size=300, seed=None)  # shuffles the items within ALL the loaded tfrecords\n",
    "\n",
    "# Iterate over all elements of the tfrecords\n",
    "for i, ex in enumerate(ds):\n",
    "    ex = tf.io.parse_single_example(ex, features=FEA_SPEC_RSP_DRUG_PAIR)\n",
    "    im_list.append(ex['image_raw'].numpy())\n",
    "    ct_list.append(ex['ctype'].numpy())\n",
    "    rsp_list.append(ex['Response'].numpy())\n",
    "    idx_list.append(ex['index'].numpy())\n",
    "    \n",
    "\n",
    "# print('Number of files:   ', len(tfr_files))\n",
    "print('Number of elements:', len(ct_list))\n",
    "print('Unique ctypes:     ', np.unique(ct_list))\n",
    "# print('bincount:          ', np.bincount(ctl_list))\n",
    "# print('\\nctype label:\\n{}'.format(ctl_list))\n",
    "# print('\\ntile id:\\n{}'.format(id_list[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggestions on data loading pipeline\n",
    "- https://github.com/tensorflow/tensorflow/issues/14857 \n",
    "- https://github.com/jkjung-avt/keras_imagenet/blob/master/utils/dataset.py\n",
    "- https://jkjung-avt.github.io/tfrecords-for-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vl_tfr_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-20bf7f44cfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The effect of repeat()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msub_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvl_tfr_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# One repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vl_tfr_files' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# The effect of repeat()\n",
    "# ----------------------\n",
    "sub_files = vl_tfr_files[:4]\n",
    "\n",
    "# One repeat\n",
    "print('\\nOne repeat:')\n",
    "repeat = 1\n",
    "shards = tf.data.Dataset.from_tensor_slices(sub_files)\n",
    "shards = shards.repeat(repeat)\n",
    "_ = [print(f) for i, f in enumerate(shards)]\n",
    "\n",
    "# Two repeat\n",
    "print('\\nTwo repeats:')\n",
    "repeat = 2\n",
    "shards = tf.data.Dataset.from_tensor_slices(sub_files)\n",
    "shards = shards.repeat(repeat)\n",
    "_ = [print(f) for i, f in enumerate(shards)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------\n",
    "# # The effect of shuffle()\n",
    "# # -----------------------\n",
    "# sub_files = vl_tfr_files[:8]\n",
    "# buffer_size = 2\n",
    "# # buffer_size = len(sub_files)\n",
    "# repeat = 1\n",
    "# seed = 0\n",
    "\n",
    "# print('\\nWithout shuffle (one repeat):')\n",
    "# shards = tf.data.Dataset.from_tensor_slices(sub_files)\n",
    "# shards = shards.repeat(repeat)\n",
    "# _ = [print(f) for i, f in enumerate(shards)]\n",
    "\n",
    "# print('\\nWith shuffle (buffer_size=2) (one repeat):')\n",
    "# shards = tf.data.Dataset.from_tensor_slices(sub_files)\n",
    "# shards = shards.repeat(repeat)\n",
    "# shards = shards.shuffle(buffer_size=buffer_size, seed=seed)\n",
    "# _ = [print(f) for i, f in enumerate(shards)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 5, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "# NOTE: New lines indicate \"block\" boundaries.\n",
    "# repeat=1\n",
    "# repeat=2\n",
    "# repeat=3\n",
    "# repeat=4\n",
    "repeat=5\n",
    "# repeat=6\n",
    "dataset = dataset.interleave(\n",
    "    lambda x: tf.data.Dataset.from_tensors(x).repeat(repeat),\n",
    "    cycle_length=2, block_length=4)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_fn = parse_tfrec_fn_rsp\n",
    "parse_fn_kwargs = {\n",
    "    \"use_tile\": \"use_tile\",\n",
    "    \"use_ge\": use_ge,\n",
    "    \"use_dd1\": use_dd1,\n",
    "    \"use_dd2\": use_dd2,\n",
    "    \"ge_scaler\": ge_scaler,\n",
    "    \"dd1_scaler\": dd1_scaler,\n",
    "    \"dd2_scaler\": dd2_scaler,\n",
    "    \"id_name\": id_name,\n",
    "    \"MODEL_TYPE\": \"categorical\",\n",
    "    \"AUGMENT\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = create_tf_data(\n",
    "    tfrecords=te_tfr_files,\n",
    "    n_concurrent_shards=16,\n",
    "    shuffle_size=8192,\n",
    "    epochs=1,\n",
    "    batch_size=512,\n",
    "    drop_remainder=False,\n",
    "    seed=None,\n",
    "    prefetch=1,\n",
    "    parse_fn=parse_fn,\n",
    "    include_meta=True,\n",
    "    **parse_fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item 0\n",
      "\ttile_image: (512, 299, 299, 3)\n",
      "\tge_data: (512, 942)\n",
      "\tdd1_data: (512, 1993)\n",
      "\tdd2_data: (512, 1993)\n",
      "\n",
      "Item 1\n",
      "\tResponse: (512,)\n",
      "\n",
      "Item 2\n",
      "\tindex: (512,)\n",
      "\tsmp: (512,)\n",
      "\tGroup: (512,)\n",
      "\tgrp_name: (512,)\n",
      "\tResponse: (512,)\n",
      "\tSample: (512,)\n",
      "\tmodel: (512,)\n",
      "\tpatient_id: (512,)\n",
      "\tspecimen_id: (512,)\n",
      "\tsample_id: (512,)\n",
      "\timage_id: (512,)\n",
      "\tctype: (512,)\n",
      "\tcsite: (512,)\n",
      "\tDrug1: (512,)\n",
      "\tDrug2: (512,)\n",
      "\ttrt: (512,)\n",
      "\taug: (512,)\n"
     ]
    }
   ],
   "source": [
    "bb = next(tf_data.take(1).__iter__())\n",
    "\n",
    "for i, item in enumerate(bb):\n",
    "    print(f\"\\nItem {i}\")\n",
    "    if isinstance(item, dict):\n",
    "        for k in item.keys():\n",
    "            print(f\"\\t{k}: {item[k].numpy().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "20\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "40\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "60\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "80\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "100\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "120\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "140\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "160\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "180\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "200\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "220\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "240\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "260\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "280\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "300\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "320\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "340\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "360\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "380\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "400\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "420\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "440\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "460\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "480\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "500\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "520\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n"
     ]
    }
   ],
   "source": [
    "batches = []\n",
    "batch_sz = []\n",
    "index = []\n",
    "for i, batch in enumerate(tf_data):\n",
    "    if (i+1)%20 == 0: print(i+1)\n",
    "    batches.append(i+1)\n",
    "    indices = batch[2][\"index\"]\n",
    "    index.append(indices)\n",
    "    batch_sz.append(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(306,), dtype=string, numpy=\n",
       " array([b'98', b'6487', b'6487', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'6487', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'6487', b'6487', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'6487', b'98', b'6487',\n",
       "        b'6487', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'6487', b'98', b'6487', b'6487', b'98', b'98',\n",
       "        b'98', b'6487', b'98', b'98', b'98', b'6487', b'98', b'98', b'98',\n",
       "        b'98', b'6487', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'6487', b'6487', b'6487', b'6487',\n",
       "        b'98', b'98', b'98', b'98', b'6487', b'6487', b'98', b'98',\n",
       "        b'6487', b'6487', b'6487', b'6487', b'98', b'98', b'98', b'6487',\n",
       "        b'98', b'6487', b'98', b'6487', b'98', b'98', b'98', b'98',\n",
       "        b'6487', b'98', b'98', b'98', b'98', b'98', b'98', b'6487', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'6487',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'6487', b'98', b'98', b'98', b'6487', b'98', b'98', b'98',\n",
       "        b'98', b'6487', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'6487', b'6487', b'98', b'98', b'98', b'98',\n",
       "        b'6487', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'6487', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'6487', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'6487', b'6487', b'98', b'6487', b'6487', b'98', b'98', b'6487',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'6487',\n",
       "        b'98', b'6487', b'6487', b'98', b'6487', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'6487', b'6487', b'6487',\n",
       "        b'98', b'98', b'98', b'98', b'6487', b'98', b'6487', b'98', b'98',\n",
       "        b'6487', b'98', b'6487', b'98', b'6487', b'98', b'98', b'98',\n",
       "        b'6487', b'98', b'6487', b'98', b'6487', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'6487', b'98', b'98', b'98', b'98', b'6487', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'6487', b'6487', b'98', b'6487', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'6487', b'98', b'6487',\n",
       "        b'6487', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98', b'98',\n",
       "        b'98', b'6487', b'6487'], dtype=object)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 4, figsize=(17, 5), constrained_layout=True)\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# img_dim = 300\n",
    "# for i, ex in enumerate(dataset.take(4)):\n",
    "#     ex = tf.io.parse_single_example(ex, features=FEA_SPEC)\n",
    "#     img = ex['image_raw']\n",
    "#     img = tf.image.decode_jpeg(img, channels=3)\n",
    "#     img = tf.cast(img, tf.float32) / 255.0\n",
    "#     img = tf.image.resize(images=img, size=(img_dim, img_dim),\n",
    "#                       method=tf.image.ResizeMethod.BILINEAR,\n",
    "#                       preserve_aspect_ratio=False)\n",
    "#     img = tf.image.random_flip_left_right(img)\n",
    "#     img = tf.image.random_flip_up_down(img)\n",
    "#     axes[i].imshow(img)\n",
    "#     axes[i].set_title('tile_id={}, ctype_label={}'.format(ex['tile_id'].numpy().decode('utf-8'), ex['ctype_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f7b8db8310a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_tfr_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# ds = tf.data.TFRecordDataset(te_tfr_files)\n",
    "# ex = next(ds.take(1).__iter__())\n",
    "# ii, oo = read_tfr_example(ex)\n",
    "# print(oo)\n",
    "\n",
    "# for i, ex in enumerate(ds.shuffle(300).take(5)):\n",
    "#     ii, oo = read_tfr_example(ex)\n",
    "\n",
    "aa = []\n",
    "ds = get_dataset(te_tfr_files)\n",
    "for i, ex in enumerate(ds.shuffle(300).take(5)):\n",
    "#     print(i)\n",
    "    aa.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(img, augment=True, img_dim=224):\n",
    "    \"\"\"\n",
    "    Prepare single image for training.\n",
    "    slideflow/model/_process_image()\n",
    "    Deotte: www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords/\n",
    "    \"\"\"\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    \n",
    "    # slideflow\n",
    "    if self.normalizer:\n",
    "        image = tf.py_function(self.normalizer.tf_to_rgb, [image], tf.int32)\n",
    "    \n",
    "    # Scale image values\n",
    "    # img = tf.cast(img, tf.float32) / 255.0  # deotte\n",
    "    # image = tf.image.per_image_standardization(image)  # slideflow\n",
    "    \n",
    "    # Resize image\n",
    "    # in case, we're using my preprocessing\n",
    "    # img = tf.image.resize(images=img, size=(img_dim, img_dim), method=tf.image.ResizeMethod.BILINEAR, preserve_aspect_ratio=False)\n",
    "    \n",
    "    if augment:\n",
    "        image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/image/random_flip_up_down\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        \n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # slideflow\n",
    "    img.set_shape([self.IMAGE_SIZE, self.IMAGE_SIZE, 3])  # slideflow\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfr_example(ex, augment=True, img_dim=224):\n",
    "    \"\"\"\n",
    "    Read and parse a single example from a tfrecord, and prepare\n",
    "    inputs and outputs for TF model training.\n",
    "    \"\"\"\n",
    "    # print('Inside read_tfr_example() ...')\n",
    "    \n",
    "    ex = tf.io.parse_single_example(ex, FEA_SPEC)\n",
    "\n",
    "    # Inputs\n",
    "    img = ex['image_raw']\n",
    "    img = prepare_image(img, augment=augment, img_dim=img_dim)  # Deotte\n",
    "    #ge_vec = tf.cast(ex['ge_vec'], tf.float32)\n",
    "    #dd_vec = tf.cast(ex['dd_vec'], tf.float32)\n",
    "    inputs = {'img': img}\n",
    "\n",
    "    # Outputs\n",
    "    #label = tf.cast(ex['label'], tf.int64)  # cats and dogs\n",
    "    ctype_label = tf.cast(ex['ctype_label'], tf.int64)\n",
    "    \n",
    "    # print('Before', ctype_label)\n",
    "    if tf.equal(ctype_label, tf.constant(10, tf.int64)):\n",
    "    # if ctype_label == 10:\n",
    "        ctype_label = tf.constant(0, tf.int64)\n",
    "    # print('After ', ctype_label)\n",
    "    \n",
    "    outputs = {'ctype_label': ctype_label}\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(files: List[str],\n",
    "                cache: bool=True,\n",
    "                shuffle: bool=False,\n",
    "                repeat: Union[bool, int]=False, \n",
    "                augment: bool=False,\n",
    "                prefetch: Optional[int]=1,\n",
    "                labeled: bool=False,\n",
    "                batch_size: int=32,\n",
    "                img_dim: int=224,\n",
    "                seed=None):\n",
    "    \"\"\" Create and return tf dataset using tfrecord files passed in files. \"\"\"\n",
    "    # print('Inside get_dataset() ...')\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=1)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # https://github.com/tensorflow/tensorflow/issues/14857\n",
    "\n",
    "    # 1. Randomly shuffle the entire data once using a MapReduce/Spark/Beam/etc. job\n",
    "    #    to create a set of roughly equal-sized files (\"shards\").\n",
    "    \n",
    "    shards = tf.data.Dataset.from_tensor_slices(files)  # data shards\n",
    "\n",
    "    # 2. In each epoch:\n",
    "    # i. Randomly shuffle the list of shard filenames, using Dataset.list_files(...).shuffle(num_shards).\n",
    "\n",
    "    shards = shards.shuffle(len(shards), seed=seed)  # shuffle the list of shard filenames\n",
    "    # shards = shards.repeat()\n",
    "    \n",
    "    # ii. Use dataset.interleave(lambda filename: tf.data.TextLineDataset(filename), cycle_length=N)\n",
    "    #     to mix together records from N different shards.\n",
    "    \n",
    "    cycle_length = 8  # default: tf.data runtime decides what it should be based on available CPU\n",
    "    # cycle_length = 1\n",
    "    block_length = 1  # default: 1\n",
    "    num_parallel_calls = None  # default: default behavior is to fetch inputs from cycle elements synchronously with no parallelism\n",
    "    dataset = shards.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x, num_parallel_reads=None),\n",
    "        cycle_length=cycle_length,\n",
    "        block_length=block_length,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "        deterministic=False\n",
    "    )\n",
    "    \n",
    "    # instead of step 2.ii.\n",
    "    # ds = tf.data.TFRecordDataset(shards, num_parallel_reads=None)\n",
    "    \n",
    "    # (ap) it seems that cache comes after TFRecordDataset()\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    # iii. Use dataset.shuffle(B) to shuffle the resulting dataset. Setting B might require some experimentation,\n",
    "    #      but you will probably want to set it to some value larger than the number of records in a single shard.\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2048)  # (ap) shuffles the examples in the relevant filenames    \n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    # (ap) why do we need repeat if we specify epochs in the model.fit() ??\n",
    "    if repeat is True:\n",
    "        dataset = dataset.repeat()\n",
    "    elif isinstance(repeat, int) and repeat > 0:\n",
    "        dataset = dataset.repeat(repeat)\n",
    "\n",
    "    # num_parallel_calls = AUTO\n",
    "    num_parallel_calls = 8\n",
    "    # num_parallel_calls = None\n",
    "    dataset = dataset.map(lambda ex: read_tfr_example(ex, augment=augment, img_dim=img_dim),\n",
    "                num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch\n",
    "    # ds = ds.prefetch(buffer_size=AUTO)\n",
    "    if prefetch is not None:\n",
    "        dataset = dataset.prefetch(buffer_size=prefetch)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(files: List[str],\n",
    "                cache: bool=True,\n",
    "                shuffle: bool=False,\n",
    "                repeat: Union[bool, int]=False, \n",
    "                augment: bool=False,\n",
    "                prefetch: Optional[int]=1,\n",
    "                labeled: bool=False,\n",
    "                batch_size: int=32,\n",
    "                img_dim: int=224,\n",
    "                seed=None):\n",
    "    \"\"\" Create and return tf dataset using tfrecord files passed in files. \"\"\"\n",
    "    # print('Inside get_dataset() ...')\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # https://github.com/tensorflow/tensorflow/issues/14857\n",
    "\n",
    "    # 1. Randomly shuffle the entire data once using a MapReduce/Spark/Beam/etc. job\n",
    "    #    to create a set of roughly equal-sized files (\"shards\").\n",
    "    \n",
    "    shards = tf.data.Dataset.from_tensor_slices(files)  # data shards\n",
    "\n",
    "    # 2. In each epoch:\n",
    "    # i. Randomly shuffle the list of shard filenames, using Dataset.list_files(...).shuffle(num_shards).\n",
    "    \n",
    "    shards = shards.shuffle(len(shards), seed=seed)  # shuffle the list of shard filenames\n",
    "    # shards = shards.repeat()\n",
    "    \n",
    "    # ii. Use dataset.interleave(lambda filename: tf.data.TextLineDataset(filename), cycle_length=N)\n",
    "    #     to mix together records from N different shards.\n",
    "    \n",
    "    cycle_length = 8  # default: tf.data runtime decides what it should be based on available CPU\n",
    "    # cycle_length = 1\n",
    "    block_length = 1  # default: 1\n",
    "    num_parallel_calls = None  # default: default behavior is to fetch inputs from cycle elements synchronously with no parallelism\n",
    "    dataset = shards.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x, num_parallel_reads=None),\n",
    "        cycle_length=cycle_length,\n",
    "        block_length=block_length,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "        deterministic=False\n",
    "    )\n",
    "    \n",
    "    # instead of step 2.ii.\n",
    "    # ds = tf.data.TFRecordDataset(shards, num_parallel_reads=None)\n",
    "    \n",
    "    # (ap) it seems that cache comes after TFRecordDataset()\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    # iii. Use dataset.shuffle(B) to shuffle the resulting dataset. Setting B might require some experimentation,\n",
    "    #      but you will probably want to set it to some value larger than the number of records in a single shard.\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2048)  # (ap) shuffles the examples in the relevant filenames    \n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    # (ap) why do we need repeat if we specify epochs in the model.fit() ??\n",
    "    if repeat is True:\n",
    "        dataset = dataset.repeat()\n",
    "    elif isinstance(repeat, int) and repeat > 0:\n",
    "        dataset = dataset.repeat(repeat)\n",
    "\n",
    "    # num_parallel_calls = AUTO\n",
    "    num_parallel_calls = 8\n",
    "    # num_parallel_calls = None\n",
    "    dataset = dataset.map(lambda ex: read_tfr_example(ex, augment=augment, img_dim=img_dim),\n",
    "                num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch\n",
    "    # ds = ds.prefetch(buffer_size=AUTO)\n",
    "    if prefetch is not None:\n",
    "        dataset = dataset.prefetch(buffer_size=prefetch)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  dict_keys(['img'])\n",
      "Outputs: dict_keys(['ctype_label'])\n",
      "\n",
      "Input features:\n",
      "img (32, 150, 150, 3)\n",
      "\n",
      "Outputs:\n",
      "ctype_label [0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Read single tfrecord and explore single example\n",
    "# -----------------------------------------------\n",
    "# ds = tf.data.TFRecordDataset(filenames=tr_tfr_files)\n",
    "# ds = ds.map(lambda ex: read_tfr_example(ex, augment=True))\n",
    "kwargs = {'batch_size': 32,\n",
    "          'shuffle': True,\n",
    "          'augment': False,\n",
    "          'repeat': False,\n",
    "          'prefetch': 5,\n",
    "          'img_dim': 150}\n",
    "\n",
    "ds = get_dataset(te_tfr_files, **kwargs)\n",
    "\n",
    "# Take an example\n",
    "ex = next(ds.take(count=1).__iter__())  # creates Dataset with at most 'count' elements from this dataset.\n",
    "# ex = next(ds.__iter__())\n",
    "print('Inputs: ', ex[0].keys())\n",
    "print('Outputs:', ex[1].keys())\n",
    "\n",
    "print('\\nInput features:')\n",
    "for i, fea_name in enumerate(ex[0].keys()):\n",
    "    print(fea_name, ex[0][fea_name].numpy().shape)\n",
    "    \n",
    "print('\\nOutputs:')\n",
    "for i, out_name in enumerate(ex[1].keys()):\n",
    "    print(out_name, ex[1][out_name].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 150, 150, 3)\n",
      "(150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(ex[0]['img'].numpy().shape)\n",
    "print(ex[0]['img'].numpy()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 150, 150, 3)\n",
      "(32,)\n",
      "(32, 150, 150, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "ds = get_dataset(te_tfr_files, **kwargs)\n",
    "for i, ex in enumerate(ds.take(2)):\n",
    "    print(ex[0]['img'].shape)\n",
    "    print(ex[1]['ctype_label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4), constrained_layout=True)\n",
    "axes = axes.ravel()\n",
    "for i, ex in enumerate(ds.take(4)):\n",
    "    idx = 31  # take an element of the current batch\n",
    "    axes[i].imshow(ex[0]['img'].numpy()[idx])\n",
    "    axes[i].set_title(ex[1]['ctype_label'].numpy()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
